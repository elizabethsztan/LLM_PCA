{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5983434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_qwen import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5910872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for Qwen/Qwen2.5-1.5B-Instruct...\n",
      "Loading model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "This is a compact 1.5B parameter model, perfect for MacBook Pro!\n",
      "Model loaded successfully!\n",
      "Model device: mps\n",
      "Model dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_qwen_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2c1dd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 1536)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d30ebb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook registered on layer 19 MLP: Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Storage for MLP inputs and outputs\n",
    "mlp_activations = {\n",
    "    'input': None,\n",
    "    'output': None\n",
    "}\n",
    "\n",
    "def mlp_hook(module, input, output):\n",
    "    \"\"\"\n",
    "    Hook function to capture MLP layer inputs and outputs\n",
    "    \n",
    "    Args:\n",
    "        module: The MLP layer\n",
    "        input: Tuple of input tensors to the layer\n",
    "        output: Output tensor from the layer\n",
    "    \"\"\"\n",
    "    # Input is a tuple, we take the first element\n",
    "    mlp_activations['input'] = input[0].detach().cpu()\n",
    "    mlp_activations['output'] = output.detach().cpu()\n",
    "    print(f\"Captured MLP activations - Input shape: {input[0].shape}, Output shape: {output.shape}\")\n",
    "\n",
    "# Register the hook on layer 19's MLP\n",
    "layer_19_mlp = model.model.layers[19].mlp\n",
    "hook_handle = layer_19_mlp.register_forward_hook(mlp_hook)\n",
    "\n",
    "print(f\"Hook registered on layer 19 MLP: {layer_19_mlp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "varrv59djd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured MLP activations - Input shape: torch.Size([1, 7, 1536]), Output shape: torch.Size([1, 7, 1536])\n",
      "\n",
      "Forward pass complete!\n",
      "MLP input shape: torch.Size([1, 7, 1536])\n",
      "MLP output shape: torch.Size([1, 7, 1536])\n"
     ]
    }
   ],
   "source": [
    "# Run a forward pass to trigger the hook\n",
    "test_prompt = \"What is the capital of France?\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(f\"\\nForward pass complete!\")\n",
    "print(f\"MLP input shape: {mlp_activations['input'].shape}\")\n",
    "print(f\"MLP output shape: {mlp_activations['output'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "pqh0qy3qie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing MLP activations...\n",
      "Input shape: torch.Size([1, 7, 1536])\n",
      "Output shape: torch.Size([1, 7, 1536])\n",
      "\n",
      "Input statistics:\n",
      "  Mean: -0.0097\n",
      "  Std: 0.6733\n",
      "  Min: -16.2031\n",
      "  Max: 19.0781\n",
      "\n",
      "Output statistics:\n",
      "  Mean: -0.0106\n",
      "  Std: 0.8530\n",
      "  Min: -18.4688\n",
      "  Max: 15.3438\n"
     ]
    }
   ],
   "source": [
    "# Now you can analyze the saved activations\n",
    "print(\"Analyzing MLP activations...\")\n",
    "print(f\"Input shape: {mlp_activations['input'].shape}\")  # Expected: [batch_size, seq_len, 1536]\n",
    "print(f\"Output shape: {mlp_activations['output'].shape}\")  # Expected: [batch_size, seq_len, 1536]\n",
    "\n",
    "# Example analysis: compute statistics\n",
    "print(f\"\\nInput statistics:\")\n",
    "print(f\"  Mean: {mlp_activations['input'].mean():.4f}\")\n",
    "print(f\"  Std: {mlp_activations['input'].std():.4f}\")\n",
    "print(f\"  Min: {mlp_activations['input'].min():.4f}\")\n",
    "print(f\"  Max: {mlp_activations['input'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nOutput statistics:\")\n",
    "print(f\"  Mean: {mlp_activations['output'].mean():.4f}\")\n",
    "print(f\"  Std: {mlp_activations['output'].std():.4f}\")\n",
    "print(f\"  Min: {mlp_activations['output'].min():.4f}\")\n",
    "print(f\"  Max: {mlp_activations['output'].max():.4f}\")\n",
    "\n",
    "# Don't forget to remove the hook when done\n",
    "# hook_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fb9l3rn28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input prompt: What is the capital of France?\n",
      "Decoded output:  is the value of the?\n",
      " The\n"
     ]
    }
   ],
   "source": [
    "# Decode the outputs to see the text predictions\n",
    "predicted_token_ids = outputs.logits.argmax(dim=-1)  # Get most likely token at each position\n",
    "decoded_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input prompt: {test_prompt}\")\n",
    "print(f\"Decoded output: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c879c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bb7423b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Beth places four whole ice cubes in a frying pan at the start of the first minute, then five at the start of the second minute and some more at the start of the third minute, but none in the fourth minute. If the average number of ice cubes per minute placed in the pan while it was frying a crispy egg was five, how many whole ice cubes can be found in the pan at the end of the third minute?\n",
      "A. 30\n",
      "B. 0\n",
      "C. 20\n",
      "D. 10\n",
      "E. 11\n",
      "F. 5\n",
      "\n",
      "Generated: Beth places four whole ice cubes in a frying pan at the start of the first minute, then five at the start of the second minute and some more at the start of the third minute, but none in the fourth minute. If the average number of ice cubes per minute placed in the pan while it was frying a crispy egg was five, how many whole ice cubes can be found in the pan at the end of the third minute?\n",
      "A. 30\n",
      "B. 0\n",
      "C. 20\n",
      "D. 10\n",
      "E. 11\n",
      "F. 5\n",
      "To solve this problem, we need to determine the total number of ice cubes that were added over the three minutes and then find out how many ice cubes are left at the end of the third minute.\n",
      "\n",
      "First, let's denote the number of ice cubes added at the start of the third minute as \\( x \\).\n",
      "\n",
      "The total number of ice cubes added in the first two minutes is:\n",
      "\\[ 4 + 5 = 9 \\]\n",
      "\n",
      "Adding the ice cubes from the third minute gives us:\n",
      "\\[ 9 + x \\]\n",
      "\n",
      "We know that the average number of ice cubes per minute over the three minutes is five. Therefore, the total number of ice cubes added over the three minutes must equal \\( 5 \\times 3 = 15 \\). So, we can set up the following equation:\n",
      "\\[ 9 + x = 15 \\]\n",
      "\n",
      "Solving for \\( x \\):\n",
      "\\[ x = 15 - 9 \\]\n",
      "\\[ x = 6 \\]\n",
      "\n",
      "So, the number of ice cubes added at the start of the third minute is 6. The total number of ice cubes added by the end of the third minute is:\n",
      "\\[ 9 + 6 = 15 \\]\n",
      "\n",
      "Therefore, the number of whole ice cubes that can be found in the pan at the end of the third minute is \\(\\boxed{15}\\). However, since the options provided do not include 15, we need to re-evaluate our approach to ensure we have interpreted the problem correctly. Given the options, the closest reasonable answer based on typical problem constraints would be the sum of the initial additions minus any additional ones that might fit the average perfectly without exceeding the given options. Since 15 does not match any exact option, the most logical interpretation considering the closest fitting scenario within the options could be:\n",
      "\n",
      "Given the options: A. 30 B. 0 C. 20 D. 10 E. 11 F. 5\n",
      "\n",
      "The correct answer based on typical logic and matching options closely is likely to be the smallest feasible sum that fits all conditions best, which aligns with the least amount adding extra beyond an average calculation. Thus, if we consider the minimum practical addition under average constraints (which logically suggests no extras except the required):\n",
      "\n",
      "\\[\n",
      "\\boxed{10}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "# Use generate() for proper text generation\n",
    "test_prompt = \"Beth places four whole ice cubes in a frying pan at the start of the first minute, then five at the start of the second minute and some more at the start of the third minute, but none in the fourth minute. If the average number of ice cubes per minute placed in the pan while it was frying a crispy egg was five, how many whole ice cubes can be found in the pan at the end of the third minute?\\nA. 30\\nB. 0\\nC. 20\\nD. 10\\nE. 11\\nF. 5\\n\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(f\"Input: {test_prompt}\")\n",
    "print(f\"Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af37e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
